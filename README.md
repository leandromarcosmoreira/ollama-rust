# ü¶ô Ollama Rust

Uma implementa√ß√£o em Rust de alta performance do Ollama, projetada para rodar modelos de linguagem (LLMs) localmente com efici√™ncia m√°xima. Este projeto utiliza o ecossistema [Candle](https://github.com/huggingface/candle) da Hugging Face para infer√™ncia e foi otimizado para ambientes conteinerizados.

---

## üöÄ Vis√£o Geral

O **Ollama Rust** oferece uma alternativa leve e extremamente r√°pida para execu√ß√£o de LLMs. Ao contr√°rio de implementa√ß√µes baseadas em frameworks mais pesados, o Ollama Rust foca em baixa lat√™ncia, gest√£o eficiente de mem√≥ria e suporte nativo a acelera√ß√£o por hardware (CUDA/Vulkan).

### ‚ú® Principais Funcionalidades

- **Infer√™ncia Nativa**: Motor de infer√™ncia puramente em Rust.
- **Acelera√ß√£o GPU**: Suporte robusto para NVIDIA CUDA.
- **API Compat√≠vel**: Interface REST compat√≠vel com o Ollama original.
- **Suporte a Modelos**: Compatibilidade com formatos GGUF e Safetensors.
- **Efici√™ncia de Recursos**: Menor pegada de mem√≥ria e CPU em compara√ß√£o com bin√°rios Go/C++.
- **WASM Ready**: Capacidade experimental de rodar em navegadores via WebAssembly.

---

## üõ†Ô∏è Arquitetura do Projeto

O projeto √© estruturado de forma modular para facilitar a manuten√ß√£o e escalabilidade:

- **`src/core/`**: O cora√ß√£o do sistema, lidando com o carregamento de pesos, gerenciamento de tokens e execu√ß√£o do modelo.
- **`src/api/`**: Servidor HTTP (Axum) que implementa os endpoints do Ollama.
- **`src/runner/`**: Orquestrador que gerencia o ciclo de vida da execu√ß√£o dos modelos.
- **`src/tokenizer/`**: Implementa√ß√µes de tokeniza√ß√£o r√°pidas e seguras.
- **`src/infra/`**: Camada de gerenciamento de hardware (detec√ß√£o de GPUs, monitoramento de VRAM).

---

## üì¶ Instala√ß√£o e Uso

### Pr√©-requisitos

- Rust 1.75 ou superior.
- (Opcional) NVIDIA CUDA Toolkit 12.x para acelera√ß√£o por GPU.
- CMake e Compiladores C/C++ (para depend√™ncias nativas).

### Compila√ß√£o

Para compilar a vers√£o otimizada com suporte a CUDA:

```bash
cargo build --release --features cuda
```

Para uma vers√£o CPU-only (mais lenta, mas universal):

```bash
cargo build --release
```

---

## üê≥ Docker

O projeto foi desenhado para ser executado em containers. O `Dockerfile` utiliza uma abordagem multi-stage para gerar imagens m√≠nimas e seguras.

### Rodando com Docker Compose

No diret√≥rio raiz das integra√ß√µes:

```bash
docker compose up -d ollama
```

### Configura√ß√µes de GPU no Docker

Certifique-se de ter o `nvidia-container-toolkit` instalado no host. O `docker-compose.yml` j√° est√° configurado para expor todas as GPUs dispon√≠veis para o container.

---

## ‚öôÔ∏è Vari√°veis de Ambiente

O **Ollama Rust** pode ser configurado atrav√©s de vari√°veis de ambiente:

| Vari√°vel | Descri√ß√£o | Padr√£o |
| :--- | :--- | :--- |
| `OLLAMA_HOST` | Host e porta para o servidor API | `0.0.0.0:11434` |
| `OLLAMA_MODELS` | Diret√≥rio para armazenamento dos modelos | `/home/ollama/.ollama/models` |
| `OLLAMA_KEEP_ALIVE` | Tempo que o modelo permanece em VRAM | `30m` |
| `OLLAMA_NUM_PARALLEL` | N√∫mero de requisi√ß√µes paralelas | `1` |
| `CUDA_VISIBLE_DEVICES` | IDs das GPUs vis√≠veis para o processo | `all` |

---

## üìù Roadmap e Contribui√ß√£o

Atualmente, o projeto foca na estabilidade da API e suporte a novos arquiteturas de modelos. Contribui√ß√µes s√£o bem-vindas!

1. Fa√ßa um Fork do projeto.
2. Crie uma Branch para sua feature (`git checkout -b feature/minha-melhoria`).
3. Fa√ßa o Commit de suas altera√ß√µes (`git commit -m 'Adiciona funcionalidade X'`).
4. Fa√ßa o Push para a Branch (`git push origin feature/minha-melhoria`).
5. Abra um Pull Request.

---

## üìú Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.

---

**Desenvolvido com ‚ù§Ô∏è pela equipe do Integracoes.**
